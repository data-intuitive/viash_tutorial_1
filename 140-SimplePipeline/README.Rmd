---
author: Data Intuitive
date: Tuesday - January 26, 2021
mainfont: Roboto Condensed
monofont: Source Code Pro
monofontoptions: Scale=0.7
monobackgroundcolor: lightgrey
title: "Creating a simple pipeline in Bash"
---

# Creating a simple pipeline in Bash

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  collapse = TRUE,
  prompt = TRUE
)

knitr::opts_knit$set(root.dir = '..')
```

In this section, we will cover how to build all the Civ6 postgame components and chain them together in a first rudimentary pipeline written in Bash. Before doing so, we first introduce the concept of a _namespace_.

## Namespaces

Once you start to develop a number of [viash] components,  grouping them (hierarchically) allows to improve maintenance of the components as it allows for separation of concern. In addition, multiple developers could group on different sets of components in parallel and later bring them together in a larger project. We call a group of components a _namespace_.

You can assign a namespace to a component by setting the `namespace` attribute in a viash config:

```yaml
functionality:
  name: some_component
  namespace: my_namespace
```

## Building a namespace
Alternatively, the namespace can be automatically inferred by structuring the components hierarchically and using the `viash ns` (read: viash namespace) command. You may have noticed that the components in the `src` directory of this repository already are structured in this manner:

```{sh}
tree src
```

With `viash ns build` you can build all the components in a namespace. If we only wish to build the Civ6 postgame components, we can specify the name of the namespace using the `-n` parameter.

```{sh}
viash ns build -n civ6_save_renderer
```

In this case, there are five components in this namespace, but multiple platforms (native, docker, nextflow) for each of them. The `viash ns` command _builds_ a _target_ for every platform it detects unless an optional `-p` is specified in the command above. By omitting the `-n`, viash will build _all_ namespaces in the `src` folder.
The `viash ns build` command is a very effective way of keeping a collection of components under `src` grouped in namespaces. Different namespaces could be split across different directories or even source repositories and then combined on the level of `viash` by specifying the _target_ directory.

Because most people will not have the necessary tools for running the different steps, we will not build the executables for the `native` platform. 

```{sh}
rm -r target
viash ns build -n civ6_save_renderer -p docker --setup > /dev/null
```

Since we have to run the _setup_ for the containers that are not just available on Docker Hub, we provide an additional `--setup` flag to let viash take care of this for us.

## Manually running executables
This is what the `target` directory looks like now:

```{sh}
tree target/
```

Please notice a few things:

- Every components has its own directory under `target/<platform>/<namespace>/`
- The `script.R`, `script.sh`, ... files are contained in the respective executables, helper files are passed at runtime.

Using the respective (containerized) tools is now as easy as, for instance,

```{sh}
target/docker/civ6_save_renderer/parse_header/parse_header -i data/AutoSave_0159.Civ6Save -o data/AutoSave_0159.yaml
```

```{sh results="asis", echo = F}
scripts/cat_format data/AutoSave_0159.yaml --cut
```


## A first pipeline in Bash

A small dataset with only a few steps from a game are stored under `data/`. We will use that as a source for the pipeline. 

With the following script:

```{sh results="asis", echo = F}
scripts/cat_format src/simple_pipeline.sh
```

Running it yields the following results.
```{bash}
src/simple_pipeline.sh
```

## Conclusions
While this bit of Bash scripting made this pipeline easy to write, there are some clear issues with it.

* All the results are produced sequentially. This strongly limits scalability as the number of samples in the datasets increases.
* A lack of parameterisation. As `input_dir` and `output_dir` are fixed, you need to modify this script every time you want to run it on a new dataset.
* No caching of results. Running the script twice will result in computing the results twice, even if they are already available.

These issues can all be fixed with some more Bash scripting (and some even by viash!), we'd be reinventing the wheel as this is all covered by Nextflow. 

In the next section, we will review some best practices when writing new components with viash, before moving on to part 2 (hint: Nextflow!).

[viash]: https://github.com/data-intuitive/viash
